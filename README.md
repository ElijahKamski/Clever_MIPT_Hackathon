# Clever_MIPT_Hackathon
Here You can find my model to classify author of a question from the famous russian quiz named "Clever"

Аналогичный монолог можно найти в начале .ipynb файла. Приятного чтения!


Перед началом, хотелось бы сказать о том, что не вошло в этом решение, но что мы обдумывали и пытались реализовать. 
> ### 50 оттенков предобработки

  Идеи были уже на этапе предобработки. Во-первых, использование готовых эмбеддингов, в частности были использованы компактные эмбеддинги navec. Помимо этого, данные были очищены от стоп-слов(что ухудшило качество классификации в данной задаче), очистка пунктуации, лемматизация слов. Последнее принесло небеольшой прирост качество, которым можно пожертвовать ради скорости работы и уменьшения вычислительной стоимости. После многих экспериментов мы решили, что в данной задаче знаки пунктуации, регистр символов и склонения несут большую смысловую нагрузку, но переписать свой "пайплайн" я не успевал, потому что исправлял модель в целом. 
  >### Война и  feature engineering

  Как я уже говорил, мы проанализировали качество моделей после разных методов предобработки, что позволило оценить вклад подобных мелочей в общее качество. К сожалению, писать feature extrator'ы мы начали слишком поздно, хотя даже простой счётчик вопросительных знаков и анализ количества заглавных букв дали прирост в пару тысячных. Кто знает, что могло бы получиться в итоге?

  >### Клуб борцов с дизбалансом классов

то борьба с несбалансированной выборкой. В ходе неё мы(Я) решили, что классические алгоритмы ***oversampling*** хорошо подойдут для генерации вопросов от экспертов. Однако оверсэплинг над векторами предложений после токенизации только сбил нас с пути тем, что встроенные методы показывали отличное качество на отложенной, но в то же время, сгерерированной выборке. Признайтесь, вам бы тоже хотелось иметь ROC-AUC и F1-Score больше 0.97. Расслабившись на время, мы взялись за голову вечером, когда осознали свою ошибку. После этого мы думали, как расширить выборку, чтобы избавиться от дисбаланса классов. Здесь могла бы подойти ***генерация подобных предложений с помощью реккурентых нейросетей***, но на это не было ни ресурсов, ни времени. После этого, мы принялись искать готовые эбмеддинги для предложений, а не слов, но возникли некоторые трудности с их имплементацией, что отняло время и потребовало нового решения. Хоть как то себя показало ***усреднение эмбедингов всех слов в предложении, а потом оверсэмплинг***. Данная техника позволила простой нейросети без LSTM и подобного получить ROC-AUC ~ 0.66. 
  Готовые модели для определения автора текста показали себя примерно так же. Далее были эксперименты с архитектурой сети и тестирования алгоритмов машинного обучения других семейств. Так мы пришли к CatBoost, который смог нас удивить своим качеством классификации "из коробки"
  >### Гарри Поттер и представление слов и предложений

Я пытался выбрать нужные методы токенизации/векторизации предложений/слов. Из-за огромного множества подходов и готовых решений проблемно попробовать всё. Первой идеей было использование предобученных эмбеддингов ELMo и RuBERT от DeepPavlov.ai, но из-за конфликтов версий внутри фреймворка, пришлось от этого отказаться. Далее был выбран ранее используемый мной navec, который и оказался в финальной версии. Обучаемые эмбеддинги не стоили прироста в скорости так, как много теряли в качестве. К сожалению, не хватило времени попробовать fasttext в данной задаче, ведь было интересно посмотреть, как он здесь отработает
>### Цветы для transfer learning

Я уже был готов самовыписаться из программистов, когда час поиска в google и яндексе не дал результатов. Очень много материалов вели к вышеупомянутому DeepPavlov.ai и его RuBERT и ELMo, встриванию которого мешало малое количество примеров и постоянные проблемы с google colab, который так и норовил "отвалиться"

>### Этюд в stacking'овых и enseble'вых тонах

У нас было 2 дня хакатона, 75 пачек печенья, 5 банок энергетиков, полпачки сигарет..
Естественно, большая часть времени ушла на очистку кода и его отладку. Некоторая - на research. Оставшегося категорически нехватало на отладку рабочей ансамблевой модели. Добавление всего лишь предсказания логистической регресии для предложения в выборку позволило увеличить метрику аж на одну десятую. Можно лишь представлять то, что бы выдавала модель, если бы к ней был добавлен еще и классификатор из KNearestNeighbours по эмбеддингам...

>### Завершение

Некоторые идеи были отвергнуты, многие остались нереализованными, а какие-то нашли своё место в нашем финальном решении. Возможно, я когда-нибудь вернусь к этому проекту, ведь проектов с NLP у меня было немного, а опыт нужен всегда. ***Было интересно работать над этой задачей, а еще интереснее, приятнее и полезнее Было бы по окончании форума и хакатона оказаться на стажировке)***


